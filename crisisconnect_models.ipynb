{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaac-ron/crisisconnectmodels/blob/main/crisisconnect_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "# --- 1. Load your COMBINED 1637-entry dataset ---\n",
        "try:\n",
        "    df = pd.read_csv('crisis_tweets.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'crisis_tweets.csv' not found.\")\n",
        "    print(\"Please ensure your combined file is uploaded and named correctly.\")\n",
        "    raise\n",
        "\n",
        "print(f\"Total entries loaded: {len(df)}\")\n",
        "\n",
        "# --- FIX 1: Remove the \"dirty\" row where the column contains its own header ---\n",
        "# This assumes the header row is the only row where 'crisis_detection' is 'crisis_detection'\n",
        "df = df[df['crisis_detection'] != 'crisis_detection'].copy() # Added .copy()\n",
        "print(f\"Entries after removing header row: {len(df)}\")\n",
        "\n",
        "# --- FIX 2: Map the *strings* \"True\" and \"False\" to 1 and 0, also handle booleans ---\n",
        "# This will create NaNs for any other junk data\n",
        "label_map = {\n",
        "    'True': 1,\n",
        "    'False': 0,\n",
        "    True: 1,  # Also handle if some are already boolean\n",
        "    False: 0  # Also handle if some are already boolean\n",
        "}\n",
        "df['label'] = df['crisis_detection'].map(label_map)\n",
        "\n",
        "# --- FIX 3: Now, drop any rows that failed the mapping (are NaN) ---\n",
        "df.dropna(subset=['label'], inplace=True)\n",
        "print(f\"Entries after dropping all bad labels: {len(df)}\")\n",
        "\n",
        "# --- Convert the clean 'label' column to int ---\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# --- 3. Define BINARY label mappings ---\n",
        "binary_label2id = {\"Non-Crisis\": 0, \"Crisis\": 1}\n",
        "binary_id2label = {0: \"Non-Crisis\", 1: \"Crisis\"}\n",
        "labels_list = [\"Non-Crisis\", \"Crisis\"]\n",
        "\n",
        "# --- 4. Split the data ---\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label'] # This will now work\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Testing samples: {len(test_df)}\")\n",
        "\n",
        "# --- 5. Convert to Hugging Face Dataset object ---\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "ds = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
        "\n",
        "print(\"Dataset created:\")\n",
        "print(ds)\n",
        "\n",
        "# --- 6. Load Tokenizer ---\n",
        "model_name = \"crisistransformers/CT-M1-Complete\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# --- 7. Tokenize Function ---\n",
        "def tokenize_function(examples):\n",
        "    # Add a check for None or NaN in tweet_text before tokenizing\n",
        "    texts = [text if isinstance(text, str) and pd.notna(text) else \"\" for text in examples['tweet_text']]\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_ds = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "# --- 8. Calculate BINARY Class Weights ---\n",
        "train_labels = np.array(train_df['label'])\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    class_weights_tensor = class_weights_tensor.to('cuda')\n",
        "print(f\"Binary Class Weights: {class_weights_tensor}\")\n",
        "\n",
        "# --- 9. Define the WeightedTrainer Class ---\n",
        "class WeightedTrainer(Trainer):\n",
        "    # Added num_items_in_batch to the signature to fix the TypeError\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=0):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# --- 10. Load the BINARY Model ---\n",
        "model_binary = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(labels_list), # 2 labels\n",
        "    id2label=binary_id2label,\n",
        "    label2id=binary_label2id\n",
        ")\n",
        "\n",
        "# --- 11. Define Training Arguments ---\n",
        "training_args_binary = TrainingArguments(\n",
        "    output_dir=\"binary_crisis_classifier\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# --- 12. Define Compute Metrics ---\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    # Use 'macro' for multi-class classification (severity) or 'binary' for binary (crisis detection)\n",
        "    # Since this function is used by both trainers, 'macro' is a safer default\n",
        "    # or you might need separate compute_metrics functions. Let's use 'macro' for now.\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\") # Changed to 'macro'\n",
        "    return {\"accuracy\": accuracy[\"accuracy\"], \"f1\": f1[\"f1\"]}\n",
        "\n",
        "# --- 13. Initialize the Trainer ---\n",
        "# Corrected FutureWarning by using processing_class\n",
        "trainer_binary = WeightedTrainer(\n",
        "    model=model_binary,\n",
        "    args=training_args_binary,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        "    tokenizer=tokenizer, # Still keep tokenizer here for now as processing_class is new\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# --- 14. Train! ---\n",
        "print(\"Starting training for the BINARY 'Gatekeeper' model...\")\n",
        "trainer_binary.train()\n",
        "\n",
        "# --- 15. Save the Final Model ---\n",
        "trainer_binary.save_model(\"my_final_binary_model\")\n",
        "print(\"Binary 'Gatekeeper' model saved to 'my_final_binary_model'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750,
          "referenced_widgets": [
            "a1c984981c65401894f7a9bb5f3f65e4",
            "a0ec6a04ad3944f2a5d49283ec665dc8",
            "cd8236d074284b8c99fa1ed40e59ea39",
            "823a54dffd644487b868591ad8dd76f5",
            "930f398e632e4d4c973a6a0f98e2145b",
            "fdad022ee52a459c8d9e78f3bc01c9e9",
            "abc861002a3644f9a295fe69879cbc85",
            "a72bc3ba7a534fb2ad560163eb56648d",
            "c7ab61d149784d8796b0c30f773c23c2",
            "e0d5e106b05640c19f8676161ff93575",
            "61fc0bedf8134ac2a5d2855d16575ec1",
            "f93f48b9577a4584be80cfd57e475187",
            "466d5c38571c42079df7ebfa90ad4d8e",
            "52fe2a66c6e1430e860bae2d309b6200",
            "d1529e7a56bc472d826857567703a2ef",
            "789d4486df1d45d289aad5444f868938",
            "b4f14ee2c0194597be4ea7d140436279",
            "c03d7e60a8e945b18425e2bec8535241",
            "80d0c7933f4a405dbb33437da2aac771",
            "047e559f86a542c4bcb2bac5b0c843c8",
            "73b6f9991df44f17817caca5bd641201",
            "9f5cf7dfb6a94133987a5a1cd739d89c"
          ]
        },
        "id": "MQcY65KfeTrk",
        "outputId": "0a741851-3f46-4a8e-871a-83384afd2a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total entries loaded: 1637\n",
            "Entries after removing header row: 1636\n",
            "Entries after dropping all bad labels: 1636\n",
            "Training samples: 1308\n",
            "Testing samples: 328\n",
            "Dataset created:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['tweet_text', 'crisis_detection', 'severity_score', 'crisis_type', 'location', 'label', '__index_level_0__'],\n",
            "        num_rows: 1308\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['tweet_text', 'crisis_detection', 'severity_score', 'crisis_type', 'location', 'label', '__index_level_0__'],\n",
            "        num_rows: 328\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1308 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1c984981c65401894f7a9bb5f3f65e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/328 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f93f48b9577a4584be80cfd57e475187"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Class Weights: tensor([0.7947, 1.3485], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at crisistransformers/CT-M1-Complete and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3787897733.py:145: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_binary = WeightedTrainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for the BINARY 'Gatekeeper' model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [410/410 05:09, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107934</td>\n",
              "      <td>0.978659</td>\n",
              "      <td>0.977043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.097383</td>\n",
              "      <td>0.972561</td>\n",
              "      <td>0.970871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.080859</td>\n",
              "      <td>0.981707</td>\n",
              "      <td>0.980357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.057759</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.986992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.056821</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.986992</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary 'Gatekeeper' model saved to 'my_final_binary_model'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'df' should still be your full, clean 1637-entry DataFrame\n",
        "# 1. Filter for crisis tweets ONLY\n",
        "df_severity = df[df['label'] == 1].copy() # label 1 = 'Crisis'\n",
        "\n",
        "print(f\"Total crisis-only tweets for severity training: {len(df_severity)}\")\n",
        "\n",
        "# 2. Create the new severity labels (Low, Medium, High, Critical)\n",
        "# We can just use the 'severity_score' column directly\n",
        "sev_labels_list = sorted(df_severity['severity_score'].unique().tolist())\n",
        "sev_label2id = {label: i for i, label in enumerate(sev_labels_list)}\n",
        "sev_id2label = {i: label for i, label in enumerate(sev_labels_list)}\n",
        "\n",
        "df_severity['label'] = df_severity['severity_score'].map(sev_label2id)\n",
        "\n",
        "print(f\"Severity labels: {sev_label2id}\")\n",
        "\n",
        "# 3. Split this new dataset\n",
        "sev_train_df, sev_test_df = train_test_split(\n",
        "    df_severity,\n",
        "    test_size=0.2, # 20% of your crisis-only data\n",
        "    random_state=42,\n",
        "    stratify=df_severity['label']\n",
        ")\n",
        "\n",
        "# 4. Convert to Hugging Face Dataset\n",
        "sev_train_dataset = Dataset.from_pandas(sev_train_df)\n",
        "sev_test_dataset = Dataset.from_pandas(sev_test_df)\n",
        "ds_sev = DatasetDict({'train': sev_train_dataset, 'test': sev_test_dataset})\n",
        "\n",
        "print(ds_sev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVYWOiNPhcrw",
        "outputId": "4820d062-e8f6-41a9-8414-fc68cdf822dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total crisis-only tweets for severity training: 607\n",
            "Severity labels: {'Critical': 0, 'High': 1, 'Low': 2, 'Medium': 3}\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['tweet_text', 'crisis_detection', 'severity_score', 'crisis_type', 'location', 'label', '__index_level_0__'],\n",
            "        num_rows: 485\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['tweet_text', 'crisis_detection', 'severity_score', 'crisis_type', 'location', 'label', '__index_level_0__'],\n",
            "        num_rows: 122\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenize the severity dataset\n",
        "# (This re-uses your 'tokenize_function')\n",
        "tokenized_ds_sev = ds_sev.map(tokenize_function, batched=True)\n",
        "\n",
        "# 2. Calculate NEW class weights for the 4 severity levels\n",
        "sev_train_labels = np.array(sev_train_df['label'])\n",
        "class_weights_sev = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(sev_train_labels),\n",
        "    y=sev_train_labels\n",
        ")\n",
        "\n",
        "# *** IMPORTANT ***\n",
        "# We must assign these new weights to the global 'class_weights_tensor'\n",
        "# variable so that our 'WeightedTrainer' class will use them.\n",
        "class_weights_tensor = torch.tensor(class_weights_sev, dtype=torch.float).to('cuda')\n",
        "\n",
        "print(f\"Severity Class Weights: {class_weights_tensor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "ed2d0984514b49179cb963e72c426d9d",
            "4f12a5680aaa454c889fa827ee8de571",
            "2c553d9856e04a52890385f5686db01c",
            "7508ecac55984f94b1bfe624beec826a",
            "4ad318784ffe484ba2eb2a8f863837a1",
            "695cf4a2ebd5489d8bc873b803c6ed11",
            "f612ee01f8f2400293a268010fd5c661",
            "d7d4702aa2744ea9b60e3f4e9174fa15",
            "7de8b1c002724d2dabe46d91a40b4d6e",
            "1b1a4c5e4f6a41b999c14b7dd7cd4e84",
            "5a62226357384e17a1dcb537ffe0d876",
            "c0b4dbb3134a46a089c9aa5da93b46de",
            "f2d2a1b608404ba08243908eabcbf0cc",
            "9de40de13d494e6d9221b4bc244214ca",
            "424565f3ad584ce89d96c1c8111970f6",
            "3aa088ba98b44e058058bbe508d4e905",
            "8e5cf06ae35349878b8f17991fe042dd",
            "08e5e0dfdd944360805b070228528117",
            "c067ab8450c94e799a39553f41d42dcd",
            "49e207d18b6c492a96e8d784af3013f5",
            "a8d02f464258493e8b8c0b1f424d00c1",
            "ddbdce6450a24b96a5a73f057f2a8600"
          ]
        },
        "id": "WYr0GSjohfix",
        "outputId": "0f954483-5c5e-4e55-8913-9b5fa9230d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/485 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed2d0984514b49179cb963e72c426d9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/122 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0b4dbb3134a46a089c9aa5da93b46de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Severity Class Weights: tensor([1.0636, 0.7723, 2.7557, 0.7132], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load a FRESH model for SEVERITY classification (4 labels)\n",
        "model_severity = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, # \"crisistransformers/CT-M1-Complete\"\n",
        "    num_labels=len(sev_labels_list), # Should be 4\n",
        "    id2label=sev_id2label,\n",
        "    label2id=sev_label2id\n",
        ")\n",
        "\n",
        "# --- Recalculate Severity Class Weights ---\n",
        "# Ensure sev_train_df is available from previous steps\n",
        "# Assuming sev_train_df['label'] contains the integer labels for the 4 severity classes\n",
        "# If not, you might need to create a 'label' column in sev_train_df mapping the severity strings to integers (0-3)\n",
        "# based on sev_label2id *before* this point.\n",
        "# Based on cell iVYWOiNPhcrw, sev_train_df already has an integer 'label' column.\n",
        "\n",
        "sev_train_labels = np.array(sev_train_df['label'])\n",
        "class_weights_sev = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(sev_train_labels),\n",
        "    y=sev_train_labels\n",
        ")\n",
        "\n",
        "# *** IMPORTANT ***\n",
        "# Re-assign these new severity weights to the global 'class_weights_tensor'\n",
        "# variable so that our 'WeightedTrainer' class will use them for this training run.\n",
        "class_weights_tensor = torch.tensor(class_weights_sev, dtype=torch.float).to('cuda')\n",
        "\n",
        "print(f\"Severity Class Weights (recalculated in this cell): {class_weights_tensor}\")\n",
        "# --- End Recalculation ---\n",
        "\n",
        "\n",
        "# 2. Define new Training Arguments\n",
        "training_args_sev = TrainingArguments(\n",
        "    output_dir=\"severity_classifier\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=8, # This smaller dataset might need more epochs\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# 3. Initialize the WeightedTrainer\n",
        "# (It will automatically use the new 'class_weights_tensor' we just set)\n",
        "trainer_sev = WeightedTrainer(\n",
        "    model=model_severity,\n",
        "    args=training_args_sev,\n",
        "    train_dataset=tokenized_ds_sev[\"train\"],\n",
        "    eval_dataset=tokenized_ds_sev[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics, # Re-uses your metric function\n",
        ")\n",
        "\n",
        "# 4. Train!\n",
        "print(\"Starting training for the SEVERITY 'Expert' model...\")\n",
        "trainer_sev.train()\n",
        "\n",
        "# 5. Save the final model\n",
        "trainer_sev.save_model(\"my_final_severity_model\")\n",
        "print(\"Severity 'Expert' model saved to 'my_final_severity_model'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "_-s-CuEIhj4S",
        "outputId": "769784e2-9e51-469f-c38f-c4078b07234a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at crisistransformers/CT-M1-Complete and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1379650802.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_sev = WeightedTrainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Severity Class Weights (recalculated in this cell): tensor([1.0636, 0.7723, 2.7557, 0.7132], device='cuda:0')\n",
            "Starting training for the SEVERITY 'Expert' model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [248/248 08:10, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.281088</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.571894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.145226</td>\n",
              "      <td>0.663934</td>\n",
              "      <td>0.649013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.018132</td>\n",
              "      <td>0.680328</td>\n",
              "      <td>0.669159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.919420</td>\n",
              "      <td>0.680328</td>\n",
              "      <td>0.678179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.859232</td>\n",
              "      <td>0.680328</td>\n",
              "      <td>0.682642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.829702</td>\n",
              "      <td>0.680328</td>\n",
              "      <td>0.682362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.818873</td>\n",
              "      <td>0.696721</td>\n",
              "      <td>0.699459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.803468</td>\n",
              "      <td>0.704918</td>\n",
              "      <td>0.701993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Severity 'Expert' model saved to 'my_final_severity_model'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEW SEVERITY MODEL dont use the one above\n"
      ],
      "metadata": {
        "id": "7j7C_tP2n3cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "# --- 1. Load ALL your data ---\n",
        "try:\n",
        "    df_main = pd.read_csv('crisis_tweets.csv')\n",
        "    df_booster_sev = pd.read_csv('booster_severity.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Make sure 'crisis_tweets.csv' and 'booster_severity.csv' are both uploaded.\")\n",
        "    raise\n",
        "\n",
        "# Combine them\n",
        "df_combined = pd.concat([df_main, df_booster_sev], ignore_index=True)\n",
        "print(f\"Total combined entries: {len(df_combined)}\")\n",
        "\n",
        "# --- 2. Clean and Filter for REAL Crises ---\n",
        "# Clean the 'crisis_detection' column\n",
        "# Explicitly map boolean values and handle the 'crisis_detection' string\n",
        "def map_crisis_detection_to_int(value):\n",
        "    if value is True:\n",
        "        return 1\n",
        "    elif value is False:\n",
        "        return 0\n",
        "    else:\n",
        "        return np.nan # Treat anything else (like the header string) as NaN\n",
        "\n",
        "df_combined['is_crisis_label'] = df_combined['crisis_detection'].apply(map_crisis_detection_to_int)\n",
        "\n",
        "\n",
        "# Now, filter for REAL crises only (is_crisis_label == 1) and drop NaNs in severity_score\n",
        "df_severity = df_combined[df_combined['is_crisis_label'] == 1].copy()\n",
        "df_severity = df_severity.dropna(subset=['severity_score']).copy() # Use .copy()\n",
        "df_severity = df_severity[df_severity['severity_score'].isin(['Low', 'Medium', 'High', 'Critical'])].copy() # Use .copy()\n",
        "\n",
        "\n",
        "print(f\"Total REAL crisis tweets for severity training: {len(df_severity)}\")\n",
        "\n",
        "# --- 3. Prepare Severity Labels ---\n",
        "sev_labels_list = sorted(df_severity['severity_score'].unique().tolist())\n",
        "sev_label2id = {label: i for i, label in enumerate(sev_labels_list)}\n",
        "sev_id2label = {i: label for i, label in enumerate(sev_labels_list)}\n",
        "df_severity['label'] = df_severity['severity_score'].map(sev_label2id)\n",
        "\n",
        "print(f\"Severity labels: {sev_label2id}\")\n",
        "\n",
        "# --- 4. Split and Create Dataset ---\n",
        "sev_train_df, sev_test_df = train_test_split(\n",
        "    df_severity,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df_severity['label']\n",
        ")\n",
        "\n",
        "# *** FIX *** Drop the original 'crisis_detection' and 'is_crisis_label' columns before converting to Dataset\n",
        "# These columns have mixed types and are not needed for the severity model training\n",
        "sev_train_df = sev_train_df.drop(columns=['crisis_detection', 'is_crisis_label'])\n",
        "sev_test_df = sev_test_df.drop(columns=['crisis_detection', 'is_crisis_label'])\n",
        "\n",
        "\n",
        "ds_sev = DatasetDict({\n",
        "    'train': Dataset.from_pandas(sev_train_df),\n",
        "    'test': Dataset.from_pandas(sev_test_df)\n",
        "})\n",
        "print(ds_sev)\n",
        "\n",
        "# --- 5. Tokenize and Get Weights ---\n",
        "# (This assumes 'tokenizer' and 'tokenize_function' are in memory)\n",
        "tokenized_ds_sev = ds_sev.map(tokenize_function, batched=True)\n",
        "\n",
        "sev_train_labels = np.array(sev_train_df['label'])\n",
        "class_weights_sev = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(sev_train_labels),\n",
        "    y=sev_train_labels\n",
        ")\n",
        "# *** This is the important part ***\n",
        "# We assign the new severity weights to the global tensor\n",
        "class_weights_tensor = torch.tensor(class_weights_sev, dtype=torch.float).to('cuda')\n",
        "print(f\"NEW Severity Class Weights: {class_weights_tensor}\")\n",
        "\n",
        "# --- 6. Load a FRESH Model for Severity ---\n",
        "# (This assumes 'model_name' is still \"crisistransformers/CT-M1-Complete\")\n",
        "model_severity = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(sev_labels_list), # 4 labels\n",
        "    id2label=sev_id2label,\n",
        "    label2id=sev_label2id\n",
        ")\n",
        "\n",
        "# --- 7. Define Training Arguments ---\n",
        "training_args_sev = TrainingArguments(\n",
        "    output_dir=\"severity_classifier_v2\", # New directory\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=8, # Train for 8 epochs on this new, larger dataset\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\", # Corrected argument name\n",
        "    save_strategy=\"epoch\", # Corrected argument name\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# --- 8. Initialize and Train ---\n",
        "# (This assumes 'WeightedTrainer' class and 'compute_metrics' function are in memory)\n",
        "trainer_sev = WeightedTrainer(\n",
        "    model=model_severity,\n",
        "    args=training_args_sev,\n",
        "    train_dataset=tokenized_ds_sev[\"train\"],\n",
        "    eval_dataset=tokenized_ds_sev[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics, # Re-uses your metric function\n",
        ")\n",
        "\n",
        "print(\"Starting training for the NEW 'Severity Expert' model (v2)...\")\n",
        "trainer_sev.train()\n",
        "\n",
        "# --- 9. Save the Final Model ---\n",
        "trainer_sev.save_model(\"my_final_severity_model_v2\")\n",
        "print(\"New 'Severity Expert' model (v2) saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757,
          "referenced_widgets": [
            "0d7453d6e5d44ef889a7fc6bbb37d118",
            "e67e1c7f32d9471e8e9c8768186a8552",
            "879df1f0dfeb44c79261707132b8c35f",
            "72905d8d81ed42049d921415ff1d2d55",
            "4b0c15f19b2a4132bb77d5b31680ba0f",
            "c6ddc0c1ce0c497088c2db42d9e1db91",
            "417996d49d8e4b748a70be70e06358aa",
            "2eae173371824b92a73a03d901b7b371",
            "d275d7b6a7104084861fa78234451c47",
            "522bad6cd64f43c0a8f4ff7224c9c4c0",
            "ef72ac7bb3874525bd628c7d838cd57b",
            "4a214bc04c6c42b8a0efb45ebebe460c",
            "cc7b6809c07049f2b31bd543d8aec6c8",
            "fcf34cedef0742ca9395da2b43ad125a",
            "f785c0f41efc473f9c4fc07422969aff",
            "6c356ff68c7b4590a01ba5bdaddd116a",
            "8a754ee1e06a46b59b4417ccf1314a1e",
            "f580b65b558c4b5687cb2a1508a9d38b",
            "a7e50abcf1b8401d81b720979718cd8e",
            "8ba215963fc243d0a24625436155b144",
            "17b2503418594809bd757cea0310aae1",
            "0d84d3cc6c864862a17a085e51db0c0b"
          ]
        },
        "id": "pjyGay4Jn8aX",
        "outputId": "93a24add-0a5d-4bd3-cdd2-aa2a1df13c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total combined entries: 1820\n",
            "Total REAL crisis tweets for severity training: 179\n",
            "Severity labels: {'Critical': 0, 'High': 1, 'Low': 2, 'Medium': 3}\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['tweet_text', 'severity_score', 'crisis_type', 'location', 'label', '__index_level_0__'],\n",
            "        num_rows: 143\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['tweet_text', 'severity_score', 'crisis_type', 'location', 'label', '__index_level_0__'],\n",
            "        num_rows: 36\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/143 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d7453d6e5d44ef889a7fc6bbb37d118"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a214bc04c6c42b8a0efb45ebebe460c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEW Severity Class Weights: tensor([1.1172, 1.0833, 0.8938, 0.9408], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at crisistransformers/CT-M1-Complete and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1673074893.py:116: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_sev = WeightedTrainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for the NEW 'Severity Expert' model (v2)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [72/72 01:43, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.378304</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.172697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.372766</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.217409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.366038</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.262446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.357335</td>\n",
              "      <td>0.305556</td>\n",
              "      <td>0.215839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.348230</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.292991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.339035</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>0.333188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.329486</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.310483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.327030</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.310483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New 'Severity Expert' model (v2) saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINAL SEVERITY MODEL\n"
      ],
      "metadata": {
        "id": "7dRYLcIgttFj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad32c1e",
        "outputId": "90574366-5b72-472f-9dfa-0a4cde4b4860"
      },
      "source": [
        "# Zip the binary model directory\n",
        "!zip -r my_final_binary_model.zip my_final_binary_model/\n",
        "\n",
        "# Zip the 4-class severity model directory\n",
        "!zip -r my_final_severity_model_4_class.zip my_final_severity_model_4_class/\n",
        "\n",
        "print(\"Model directories zipped. You can now download 'my_final_binary_model.zip' and 'my_final_severity_model_4_class.zip' from the files sidebar.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: my_final_binary_model/ (stored 0%)\n",
            "  adding: my_final_binary_model/config.json (deflated 52%)\n",
            "  adding: my_final_binary_model/training_args.bin (deflated 53%)\n",
            "  adding: my_final_binary_model/tokenizer.json (deflated 81%)\n",
            "  adding: my_final_binary_model/merges.txt (deflated 51%)\n",
            "  adding: my_final_binary_model/tokenizer_config.json (deflated 75%)\n",
            "  adding: my_final_binary_model/special_tokens_map.json (deflated 84%)\n",
            "  adding: my_final_binary_model/vocab.json (deflated 57%)\n",
            "  adding: my_final_binary_model/model.safetensors (deflated 7%)\n",
            "  adding: my_final_severity_model_4_class/ (stored 0%)\n",
            "  adding: my_final_severity_model_4_class/config.json (deflated 52%)\n",
            "  adding: my_final_severity_model_4_class/training_args.bin (deflated 53%)\n",
            "  adding: my_final_severity_model_4_class/tokenizer.json (deflated 81%)\n",
            "  adding: my_final_severity_model_4_class/merges.txt (deflated 51%)\n",
            "  adding: my_final_severity_model_4_class/tokenizer_config.json (deflated 75%)\n",
            "  adding: my_final_severity_model_4_class/special_tokens_map.json (deflated 84%)\n",
            "  adding: my_final_severity_model_4_class/vocab.json (deflated 57%)\n",
            "  adding: my_final_severity_model_4_class/model.safetensors (deflated 7%)\n",
            "Model directories zipped. You can now download 'my_final_binary_model.zip' and 'my_final_severity_model_4_class.zip' from the files sidebar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "# --- 1. Load your combined 1637-entry dataset ---\n",
        "# (This assumes 'df_combined' is still in memory from the last run)\n",
        "# If not, re-load your crisis_tweets.csv and booster_severity.csv here.\n",
        "print(f\"Total combined entries: {len(df_combined)}\")\n",
        "\n",
        "# --- 2. Clean and Filter for REAL Crises ---\n",
        "# (This is the same cleaning logic as before)\n",
        "label_map = {\n",
        "    'True': 1, 'False': 0,\n",
        "    True: 1, False: 0,\n",
        "    'crisis_detection': np.nan\n",
        "}\n",
        "df_combined['is_crisis_label'] = df_combined['crisis_detection'].map(label_map)\n",
        "df_combined = df_combined.dropna(subset=['is_crisis_label'])\n",
        "df_severity = df_combined[df_combined['is_crisis_label'] == 1].copy()\n",
        "df_severity = df_severity.dropna(subset=['severity_score'])\n",
        "df_severity = df_severity[df_severity['severity_score'].isin(['Low', 'Medium', 'High', 'Critical'])]\n",
        "\n",
        "print(f\"Total REAL crisis tweets: {len(df_severity)}\")\n",
        "\n",
        "# --- 3. Prepare 4-Class Labels ---\n",
        "# (This is the same 4-class setup as the last failed run)\n",
        "sev_labels_list = sorted(df_severity['severity_score'].unique().tolist())\n",
        "sev_label2id = {label: i for i, label in enumerate(sev_labels_list)}\n",
        "sev_id2label = {i: label for i, label in enumerate(sev_labels_list)}\n",
        "df_severity['label'] = df_severity['severity_score'].map(sev_label2id)\n",
        "\n",
        "print(f\"Severity labels: {sev_label2id}\")\n",
        "\n",
        "# --- 4. Split and Create 4-Class Dataset ---\n",
        "sev_train_df, sev_test_df = train_test_split(\n",
        "    df_severity,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df_severity['label']\n",
        ")\n",
        "\n",
        "ds_sev_4_class = DatasetDict({\n",
        "    'train': Dataset.from_pandas(sev_train_df[['tweet_text', 'label']]),\n",
        "    'test': Dataset.from_pandas(sev_test_df[['tweet_text', 'label']])\n",
        "})\n",
        "print(ds_sev_4_class)\n",
        "\n",
        "# --- 5. Tokenize and Get 4-Class Weights ---\n",
        "tokenized_ds_sev_4_class = ds_sev_4_class.map(tokenize_function, batched=True)\n",
        "\n",
        "sev_train_labels = np.array(sev_train_df['label'])\n",
        "class_weights_sev_4 = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(sev_train_labels),\n",
        "    y=sev_train_labels\n",
        ")\n",
        "# *** Update the global tensor with these 4-class weights ***\n",
        "class_weights_tensor = torch.tensor(class_weights_sev_4, dtype=torch.float).to('cuda')\n",
        "print(f\"4-Class Severity Class Weights: {class_weights_tensor}\")\n",
        "\n",
        "# --- 6. Load a FRESH Model for 4-Class Classification ---\n",
        "model_severity_4_class = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, # \"crisistransformers/CT-M1-Complete\"\n",
        "    num_labels=len(sev_labels_list), # Should be 4\n",
        "    id2label=sev_id2label,\n",
        "    label2id=sev_label2id\n",
        ")\n",
        "\n",
        "# --- 7. Define Training Arguments (with MORE EPOCHS) ---\n",
        "training_args_sev_4 = TrainingArguments(\n",
        "    output_dir=\"severity_classifier_4_class\", # New directory\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=15,  # <<--- TRAINING FOR 15 EPOCHS\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# --- 8. Initialize and Train ---\n",
        "# (This re-uses your 'WeightedTrainer' and 'compute_metrics' functions)\n",
        "trainer_sev_4 = WeightedTrainer(\n",
        "    model=model_severity_4_class,\n",
        "    args=training_args_sev_4,\n",
        "    train_dataset=tokenized_ds_sev_4_class[\"train\"],\n",
        "    eval_dataset=tokenized_ds_sev_4_class[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Starting training for the 4-Class 'Severity Expert' (15 epochs)...\")\n",
        "trainer_sev_4.train()\n",
        "\n",
        "# --- 9. Save the Final Model ---\n",
        "trainer_sev_4.save_model(\"my_final_severity_model_4_class\")\n",
        "print(\"New 4-Class 'Severity Expert' model saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "13f3733f0c6b4e188382bbf2f49c1afb",
            "7a25d57f8bc2478e8da0f7d3a974b22d",
            "b26e94ff7cde4ef59be6cfe42381b7e2",
            "59bc20d9a0f14f0f92cb4568d39c0ed1",
            "a28e3e4bfc1c490f84849e7224737b20",
            "2e202ac1741e481688b3140c0b7170d1",
            "dbcd9d3c029444988f2fb8b73f48cd52",
            "91b47e08dc5a42b19e93ea4ad0057ce0",
            "98c5484211cc4189b2a78aa039870779",
            "712b0c0f64c94549a83dfd53bb2f8850",
            "ad500e1f52d3465194fa6ed828ce6adb",
            "0fffb1e9ffa44af2b14cc98bbe2f0daf",
            "536f0a3cfb914d5189ef8149ed3aba8d",
            "4fa92e11c6c74369be0237119543c30c",
            "284551a2517b42c68846f2a64cd5db9e",
            "c925d739ec0944ccaa873a3c995dd720",
            "8aaa5598bd564afa9b608f39ec864297",
            "7e54c5a49cc4498d86cb05062873b193",
            "bf46ea7dd8ec457e8cc9f8d7e60beed7",
            "76cf4d097d46458b9abddb81664dccba",
            "9ca99ca2d2b94e9e9ae6b5b4d3ff8a09",
            "723ed972d6f3437d83dbbc0b8ea5e00e"
          ]
        },
        "id": "lEstAMXeq-py",
        "outputId": "01bb0e50-b2eb-4689-bb1f-75816de03bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total combined entries: 1815\n",
            "Total REAL crisis tweets: 786\n",
            "Severity labels: {'Critical': 0, 'High': 1, 'Low': 2, 'Medium': 3}\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['tweet_text', 'label', '__index_level_0__'],\n",
            "        num_rows: 628\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['tweet_text', 'label', '__index_level_0__'],\n",
            "        num_rows: 158\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3186642634.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_combined['is_crisis_label'] = df_combined['crisis_detection'].map(label_map)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/628 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13f3733f0c6b4e188382bbf2f49c1afb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/158 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fffb1e9ffa44af2b14cc98bbe2f0daf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4-Class Severity Class Weights: tensor([1.0753, 0.8307, 1.8690, 0.7512], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at crisistransformers/CT-M1-Complete and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3186642634.py:94: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_sev_4 = WeightedTrainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for the 4-Class 'Severity Expert' (15 epochs)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [600/600 05:42, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.252771</td>\n",
              "      <td>0.518987</td>\n",
              "      <td>0.521522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.051672</td>\n",
              "      <td>0.664557</td>\n",
              "      <td>0.691886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.848189</td>\n",
              "      <td>0.670886</td>\n",
              "      <td>0.674267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.721425</td>\n",
              "      <td>0.715190</td>\n",
              "      <td>0.727750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.626280</td>\n",
              "      <td>0.778481</td>\n",
              "      <td>0.786771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.609760</td>\n",
              "      <td>0.765823</td>\n",
              "      <td>0.771847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.621723</td>\n",
              "      <td>0.765823</td>\n",
              "      <td>0.774699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.586703</td>\n",
              "      <td>0.759494</td>\n",
              "      <td>0.765156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.564218</td>\n",
              "      <td>0.765823</td>\n",
              "      <td>0.773042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.552707</td>\n",
              "      <td>0.810127</td>\n",
              "      <td>0.812226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.559101</td>\n",
              "      <td>0.803797</td>\n",
              "      <td>0.807174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.534856</td>\n",
              "      <td>0.816456</td>\n",
              "      <td>0.821680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.607500</td>\n",
              "      <td>0.570994</td>\n",
              "      <td>0.822785</td>\n",
              "      <td>0.824358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.607500</td>\n",
              "      <td>0.566036</td>\n",
              "      <td>0.810127</td>\n",
              "      <td>0.812095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.607500</td>\n",
              "      <td>0.561342</td>\n",
              "      <td>0.791139</td>\n",
              "      <td>0.796109</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New 4-Class 'Severity Expert' model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2b2b6d0",
        "outputId": "96d2f431-b689-48a4-c211-cdd0b66d75e7"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# List of directories containing intermediate checkpoints to delete\n",
        "# Added 'severity_classifier_4_class' from the last training run\n",
        "checkpoint_dirs_to_delete = [\n",
        "    \"severity_classifier_4_class\"\n",
        "]\n",
        "\n",
        "print(\"Deleting intermediate checkpoint directories...\")\n",
        "\n",
        "for dir_name in checkpoint_dirs_to_delete:\n",
        "    if os.path.exists(dir_name):\n",
        "        try:\n",
        "            shutil.rmtree(dir_name)\n",
        "            print(f\"Deleted: {dir_name}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error deleting directory {dir_name}: {e}\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {dir_name}\")\n",
        "\n",
        "print(\"Deletion complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting intermediate checkpoint directories...\n",
            "Deleted: severity_classifier_4_class\n",
            "Deletion complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DICgImcep8W3",
        "outputId": "583a0100-3a7e-4167-8868-789414e8e90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "182257e3",
        "outputId": "458b8077-b312-4c62-f452-9a959a4ae98c"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# List of directories containing intermediate checkpoints\n",
        "checkpoint_dirs = [\n",
        "    \"binary_crisis_classifier\",\n",
        "    \"severity_classifier\",\n",
        "    \"severity_classifier_v2\"\n",
        "]\n",
        "\n",
        "print(\"Deleting intermediate checkpoint directories...\")\n",
        "\n",
        "for dir_name in checkpoint_dirs:\n",
        "    if os.path.exists(dir_name):\n",
        "        try:\n",
        "            shutil.rmtree(dir_name)\n",
        "            print(f\"Deleted: {dir_name}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error deleting directory {dir_name}: {e}\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {dir_name}\")\n",
        "\n",
        "print(\"Deletion complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting intermediate checkpoint directories...\n",
            "Deleted: binary_crisis_classifier\n",
            "Deleted: severity_classifier\n",
            "Deleted: severity_classifier_v2\n",
            "Deletion complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "import json\n",
        "\n",
        "# --- 1. Load the \"Gatekeeper\" (Binary Classifier)  ---\n",
        "print(\"Loading the 'Gatekeeper' (Binary) model...\")\n",
        "gatekeeper_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"my_final_binary_model\"\n",
        ")\n",
        "\n",
        "# --- 2. Load the \"Severity Expert\" (4-Class Classifier)  ---\n",
        "print(\"Loading the 'Severity Expert' model...\")\n",
        "# --- UPDATED: Use the new 4-class severity model ---\n",
        "severity_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"my_final_severity_model_4_class\" # Changed model path\n",
        ")\n",
        "\n",
        "# --- 3. Load the \"Location Extractor\" (NER)  ---\n",
        "print(\"Loading the 'Location Extractor' (NER) model...\")\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=\"dslim/bert-base-NER\",\n",
        "    grouped_entities=True\n",
        ")\n",
        "\n",
        "print(\"All models loaded. Pipeline is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEQwGoyvl0zR",
        "outputId": "792d6137-9f16-4f23-b31d-8627b8d63b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the 'Gatekeeper' (Binary) model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the 'Severity Expert' model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the 'Location Extractor' (NER) model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All models loaded. Pipeline is ready!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_crisis_tweet(text):\n",
        "    \"\"\"\n",
        "    Analyzes a tweet using the 2-step pipeline.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Run the \"Gatekeeper\" model ---\n",
        "    gatekeeper_result = gatekeeper_pipeline(text)[0]\n",
        "\n",
        "    # --- Step 2: Check the Gatekeeper's decision ---\n",
        "    if gatekeeper_result['label'] == 'Non-Crisis':\n",
        "        # If it's not a crisis, we stop here.\n",
        "        return {\n",
        "            \"tweet_text\": text,\n",
        "            \"is_crisis\": False,\n",
        "            \"severity\": \"N/A\",\n",
        "            \"classification_confidence\": round(gatekeeper_result['score'], 4),\n",
        "            \"extracted_locations\": [] # No need to run NER\n",
        "        }\n",
        "\n",
        "    # --- Step 3: If it IS a crisis, run the \"Expert\" models ---\n",
        "\n",
        "    # A) Run the Severity Expert\n",
        "    severity_result = severity_pipeline(text)[0]\n",
        "    severity = severity_result['label']\n",
        "\n",
        "    # B) Run the Location Extractor\n",
        "    ner_results = ner_pipeline(text)\n",
        "    locations = []\n",
        "    for entity in ner_results:\n",
        "        if entity['entity_group'] == 'LOC':\n",
        "            clean_loc = re.sub(r'[#@]', '', entity['word'])\n",
        "            locations.append(clean_loc.strip())\n",
        "\n",
        "    # C) Format the final output\n",
        "    return {\n",
        "        \"tweet_text\": text,\n",
        "        \"is_crisis\": True,\n",
        "        \"severity\": severity,\n",
        "        \"classification_confidence\": round(severity_result['score'], 4),\n",
        "        \"extracted_locations\": list(set(locations))\n",
        "    }"
      ],
      "metadata": {
        "id": "sOEn5fFtl5c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Let's test it! ---\n",
        "\n",
        "# A real crisis\n",
        "test_tweet_1 = \"Horrible multi-car pile-up on the A104 highway near Naivasha. Traffic is backed up for kilometers. Avoid the area.\"\n",
        "\n",
        "# Figurative language (non-crisis)\n",
        "test_tweet_2 = \"My CPU is on fire trying to render this 4K video project.  #TechProbs\"\n",
        "\n",
        "# A real, but lower-severity, crisis\n",
        "test_tweet_3 = \"Just felt a small shake in Nakuru! The house was rattling for a second. #earthquake\"\n",
        "\n",
        "# Figurative language\n",
        "test_tweet_4 = \"That new Sauti Sol album just dropped and it's an absolute bombshell!\"\n",
        "\n",
        "# A critical event\n",
        "test_tweet_5 = \"Active shooter reported at Two Rivers Mall. Police are on the scene. Stay away from the area!\"\n",
        "\n",
        "\n",
        "print(json.dumps(analyze_crisis_tweet(test_tweet_1), indent=2))\n",
        "print(\"---\")\n",
        "print(json.dumps(analyze_crisis_tweet(test_tweet_2), indent=2))\n",
        "print(\"---\")\n",
        "print(json.dumps(analyze_crisis_tweet(test_tweet_3), indent=2))\n",
        "print(\"---\")\n",
        "print(json.dumps(analyze_crisis_tweet(test_tweet_4), indent=2))\n",
        "print(\"---\")\n",
        "print(json.dumps(analyze_crisis_tweet(test_tweet_5), indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKSt15mgmEbg",
        "outputId": "566a6514-87ca-4014-93dc-7411abf0c576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"tweet_text\": \"Horrible multi-car pile-up on the A104 highway near Naivasha. Traffic is backed up for kilometers. Avoid the area.\",\n",
            "  \"is_crisis\": true,\n",
            "  \"severity\": \"High\",\n",
            "  \"classification_confidence\": 0.8142,\n",
            "  \"extracted_locations\": [\n",
            "    \"A1\",\n",
            "    \"Naivasha\"\n",
            "  ]\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"tweet_text\": \"My CPU is on fire trying to render this 4K video project. \\ud83d\\udcbb\\ud83d\\udd25 #TechProbs\",\n",
            "  \"is_crisis\": false,\n",
            "  \"severity\": \"N/A\",\n",
            "  \"classification_confidence\": 0.9898,\n",
            "  \"extracted_locations\": []\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"tweet_text\": \"Just felt a small shake in Nakuru! The house was rattling for a second. #earthquake\",\n",
            "  \"is_crisis\": true,\n",
            "  \"severity\": \"Medium\",\n",
            "  \"classification_confidence\": 0.8383,\n",
            "  \"extracted_locations\": []\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"tweet_text\": \"That new Sauti Sol album just dropped and it's an absolute bombshell!\",\n",
            "  \"is_crisis\": false,\n",
            "  \"severity\": \"N/A\",\n",
            "  \"classification_confidence\": 0.9897,\n",
            "  \"extracted_locations\": []\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"tweet_text\": \"Active shooter reported at Two Rivers Mall. Police are on the scene. Stay away from the area!\",\n",
            "  \"is_crisis\": true,\n",
            "  \"severity\": \"High\",\n",
            "  \"classification_confidence\": 0.773,\n",
            "  \"extracted_locations\": [\n",
            "    \"Two Rivers Mall\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}